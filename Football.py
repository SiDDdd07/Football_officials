# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18Dq5tzxNb-tzkSm0wkw5OiIaABzwiaXL
"""

import gdown
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
# Google Drive shareable link
file_id = "1uldHEOpwsUbZahaDGXpHD1LwEcsquwA4"
url = f"https://drive.google.com/uc?id={file_id}"

# Download to local file
output = "match_commentary.csv"
gdown.download(url, output, quiet=False)

# Read the CSV
df = pd.read_csv(output)

df.head()

df.dropna(subset=["events", "summary"], inplace=True)

def clean_text(text):
    text = re.sub(r"Hello.*?(coverage|commentary) of", "", text, flags=re.IGNORECASE)
    text = re.sub(r"\s+", " ", text).strip()
    return text

df["events"] = df["events"].apply(clean_text)
df["summary"] = df["summary"].apply(str.strip)

df.reset_index(drop=True, inplace=True)

df["events_len"] = df["events"].str.split().apply(len)
df["summary_len"] = df["summary"].str.split().apply(len)

plt.figure(figsize=(12, 5))
sns.histplot(df["events_len"], kde=True, bins=30)
plt.title("Distribution of Event Commentary Length (in words)")
plt.xlabel("Word Count")
plt.ylabel("Frequency")
plt.show()

plt.figure(figsize=(12, 5))
sns.histplot(df["summary_len"], kde=True, color="orange", bins=30)
plt.title("Distribution of Summary Length (in words)")
plt.xlabel("Word Count")
plt.ylabel("Frequency")
plt.show()

from sklearn.model_selection import train_test_split
from datasets import Dataset

# Split your dataframe
train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)

# Convert to HuggingFace datasets
train_dataset = Dataset.from_pandas(train_df[["events", "summary"]], preserve_index=False)
val_dataset = Dataset.from_pandas(val_df[["events", "summary"]], preserve_index=False)

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

training_args = Seq2SeqTrainingArguments(  # Use Seq2SeqTrainingArguments
    output_dir="./results",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    eval_strategy="no",  # Changed from evaluation_strategy
    learning_rate=5e-5,
    num_train_epochs=3,
    logging_steps=50,
    save_strategy="no",
    predict_with_generate=True,  # Important for seq2seq
    fp16=True,
    dataloader_num_workers=0,
    remove_unused_columns=False,
    report_to=[],
)

model_name = "sshleifer/distilbart-cnn-12-6"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
def preprocess(example):
    model_inputs = tokenizer(
        example["events"],
        max_length=1024,
        truncation=True,
        padding="max_length"
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            example["summary"],
            max_length=128,
            truncation=True,
            padding="max_length"
        )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Tokenize both splits
train_tokenized = train_dataset.map(preprocess, batched=True)
val_tokenized = val_dataset.map(preprocess, batched=True)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=val_tokenized
)

trainer.train()

import os
os.environ["WANDB_DISABLED"] = "true"

metrics = trainer.evaluate()
print(metrics)





